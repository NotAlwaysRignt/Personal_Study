### 网络设备驱动编程特点
网络设备与块设备和字节设备有所不同,应用层并不是主要通过设备文件和设备交互,而是通过数据结构 `sk_buff` 读取缓冲区的内容来得到报文

#### sk_buff
`sk_buff` 就是一个存放报文的缓冲区,比如以太网报文,去掉报头和尾部后就是 IP 报文, IP 报文再去掉报头就是 TCP 报文,这些报文信息都存放在 `sk_buff` 这个数据结构中

TCP,IP 层代码,包括解包等,属于协议栈代码,它们实现在内核中,但它们不属于驱动程序,驱动程序应该是负责处理数据链路层和物理层的

### 网络设备驱动分层
网络设备与媒介层对应物理层,网络协议接口层,网络设备接口层,设备驱动功能层对应数据链路层
#### 网络协议接口层
给上层协议提供统一的数据包收发接口,不管是 ARP 还是 IP 协议,都通过 `dev_queue_xmit`函数发送数据,通过`netif_rx`(netif表示 net interface)函数接收数据. 网络协议接口层通过提供接口使得上层协议独立于具体的设备,即上层协议实现时只管调用接口,不用管下层设备

为网络设别提供的接口举例:
* 激活设备发送队列`netif_start_queue`
* 停止设备发送队列`netif_stop_queue`
* 重新启动设备发送队列 `netif_awake_queue`

#### 网络设备接口层
为千变万化的网络设备定义统一的,抽象的数据结构`net_device`(可类比字符设备 cdev)
`net_device`结构体是对一个网络硬件设备的抽象,通过为结构体数据成员赋值,包括硬件的信息,如名称,IO 地址以及一些函数指针,即可实现内核与具体硬件操作函数的挂接

头文件`linux/net_device.h`

#### 设备驱动功能层
设备驱动功能层的主要工作就是实现函数, 并赋值给 `net_device`结构体内的相应函数指针成员. 这其实就是 Linux 驱动开发的特点,开发人员根据 Linux 的规范接口来实现相应函数,以适配 Linux 的架构

#### 网络设备与媒介层
和硬件具体打交道的一层,主要是一些访问网络设备内部寄存器的操作

### net_device 介绍
Linux 用`struct net_device` 来抽象一个网络设备,一个网络设备(如网卡)对应一个 net_device 成员
`net_device`内部有很多函数指针类型的成员,如`open`,`stop`,`hard_start_xmit`等,这些指针指向的函数就是操纵硬件的程序,由驱动开发人员实现
在新版的内核中`open`,`close`,`hard_start_xmit`等函数不再是`net_device`的成员,`net_device`把这些函数指针成员放进了新增成员`net_dev_ops`里,`net_dev_ops`的类型是`struct net_device_ops`,`ndo_open`与原来的`open`等效,`ndo_start_xmit`与原来的`hard_start_xmit`等效,其它函数指针基本上也是名字前加 ndo 前缀,不再赘述

### sk_buff 介绍
数据结构 sk_buff 是一个链表成员,用来存放一块数据,并记录这块数据的头部,尾部等信息
一个 sk_buff 不一定存放完整的数据包信息,数据包太大时是可以分片的
sk_buff 通常也叫做 skb,下面用 skb 称呼
四个重要数据成员
* next 指向下一个 sk_buff
* pre 指向前一个 sk_buff
* head 缓冲区头部开始地址
* data 数据区开始地址
* tail 数据区结束地址
* end 缓冲区结束地址
#### net_device 是怎么被调用发包的
我们只要明确,顶层socket调用send发包,不管是TCP还是UDP,到了内核,最终一定是调用`dev->hard_start_xmit`来发送skb包的(dev 即 `struct net_device`),最新版linux内核应该是调用`dev->net_dev_ops->ndo_start_xmit`这个函数(参考: http://blog.sina.com.cn/s/blog_4c4eac770100uvzz.html)

`ndo_start_xmit`指向的驱动开发者实现的函数指针时,一般会在这个函数里调用中断,然后加锁收发包
#### net_device 和 sk_buff的关系
一个 `struct net_device`可以调度一个数据包队列:
![img/skb_queue1.png]
有的网络设备支持设备队列,即一个网卡可以拥有多个队列:
![img/skb_queue2.png]
我们从上一层开始分析,看看 skb(sk_buff)
Qdisc(排队规则)是queueingdiscipline的简写,内核如果要TXQ代表网卡的队列
![img/qdisc.png]
参考文章:https://blog.csdn.net/tenfyguo/article/details/8777436

#### 相关函数
* alloc_skb 分配内存返回一个 skbuff 结构,返回指针
* kfree_skb 释放 sk_buff 数据结

处理 sk_buff 也有专门的函数,其实是数据指针后移,并不是重新分配了内存
* skb_put 数据区末端添加某协议的尾部,比如当前层要把数据交给上一层时
* skb_push 添加协议头部,其实是数据指针前移,并不是重新分配了内存
* skb_pull 把头部去掉
* skb_strim 把尾部去掉

sk_buff缓冲区链表操作
* skb_orphan 将一个缓冲区结构体变成孤立的 skb
* skb_queue_init 初始化 sk_buff_head 结构体
* skb_queue_head 在链表头添加一个 sk_buff 结构
* skb_queue_tail 在缓冲区尾部添加新的 skb
* skb_dequeue 在链表头部移除一个 skb
* skb_dequeue_tail 在链表尾部移除一个 skb
* skb_queue_purge 清空一个 skb 链表
* skb_append 在某个 skb 后添加一个缓冲区


### 开发相关函数
###### ioremap
ioremap 是一个宏,定义在asm/io.h内,定义:
```cpp
#define ioremap(cookie,size)      __ioremap(cookie,size,0)
//__ioremap函数原型为(arm/mm/ioremap.c)
void __iomem * __ioremap(unsigned long phys_addr, size_t size, unsigned long flags);
```
* phys_addr：要映射的起始的IO地址
* size：要映射的空间的大小
* flags：要映射的IO空间和权限有关的标志

该函数返回映射后的内核虚拟地址(3G-4G). 接着便可以通过读写该返回的内核虚拟地址去访问之这段I/O内存资源。
###### ether_setup
原型:`void ether_setup(struct net_device *dev)`
初始化`struct net_device`字段,将这个 dev 设置成以太网类型(对一部分成员变量设置初值)

### 网卡收发包过程(以 dm9000 为例)
#### 发送过程
网卡发包的时候只要把以一个 skb 的数据从内核搬运到网卡缓冲区中,并告诉数据长度,然后调用`dm9000_send_packet`执行启动,硬件会自动把缓冲区的内容发送,发送过程不用软件控制
发送完毕后,会有一个中断到来,通知事件完成,在软件中表现为`dm9000_send_packet`返回,之后我们就可以free 释放掉这个 skb 了
整个发送过程中要加锁和关中断.即同一个时刻只能有一个进程在执行发送网卡操作

#### timeout处理
先通知上层停止发包,然后重置网卡,重新初始化,再通知上层可以继续发包

#### 收包 dm9000_rx
收包的函数`dm9000_rx`是在中断函数中被调用的,由此可以看出收包操作是通过中断来触发的.
首先会检查网卡缓冲区(注意网卡缓冲区和内存是有区别的,网卡缓冲区属于网卡,独立于内存),如果准备好了,则开始把缓冲区的内容写到内存中来,具体过程是,先在内存中分配一个 skb 结构,然后解析缓冲区的数据,通过 `skb_put`,`skb_pull`等操作逐步将数据填充 skb 的数据成员.
skb构造好以后,调用`netif_rx(skb)`把 skb 放到 skb 的队列尾部,然后通知上层来收包


另外值得一提的是网卡的收包机制 NAPI 模式,网卡收包有两种机制,轮询和中断,轮询是使用定时器定时去查看是否有包到来,这样处理效率并不高,所以一般使用中断为主
传统的收包方式是网卡每收到一帧就产生一个中断去收包,在高速网卡中,每秒几千个中断可能导致整个系统处于饥饿状态(饥饿状态的意思是系统忙于处理中断程序，没有时间执行其他程序)
为了解决以上问题,使用 NAPI 机制,将中断和轮询结合,当收到包后,首先也会有一个中断事件给内核,中断事件到来后会把中断给关闭,同时将适配器放到一个轮询表上
等中断处理完后,只要适配器上还有分组(packet)要处理,就会一直对轮询表上的设备进行轮询,这个期间收到的包放在适配器缓冲区中,不需要中断也会被收到, 而中断是关闭的,因此不会有之前那种大量中断所造成的问题
